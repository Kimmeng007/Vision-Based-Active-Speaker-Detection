{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Library\n"
      ],
      "metadata": {
        "id": "FY8HT3cFVYmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-kgoJ1I33gLI",
        "outputId": "28049b31-79b7-4adb-bfdb-507195843806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Using cached ultralytics-8.3.177-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.16.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Using cached ultralytics_thop-2.0.15-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Using cached ultralytics-8.3.177-py3-none-any.whl (1.0 MB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Using cached ultralytics_thop-2.0.15-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.177 ultralytics-thop-2.0.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install filterpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uV67rHuM4JyN",
        "outputId": "d1e24a0d-47eb-42ab-ed34-3b4fab32c5bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting filterpy\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/178.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from filterpy) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from filterpy) (1.16.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from filterpy) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->filterpy) (1.17.0)\n",
            "Building wheels for collected packages: filterpy\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110460 sha256=98072f05b90a6428cb27a034f76b205ad6efc999feae37c21e6d281c88489b29\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/dc/3c/e12983eac132d00f82a20c6cbe7b42ce6e96190ef8fa2d15e1\n",
            "Successfully built filterpy\n",
            "Installing collected packages: filterpy\n",
            "Successfully installed filterpy-1.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/abewley/sort.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Llao0Xj64O0N",
        "outputId": "53757bae-2fec-4968-b8e7-40b38070ba90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'sort' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N87rXkYbU6xa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from collections import deque, defaultdict\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "# face detectors\n",
        "from sort.sort import Sort\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Webcam on google colab\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Config"
      ],
      "metadata": {
        "id": "uh-C6JgnYJrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VIDEO_SOURCE = \"1D_Video.mp4\"  # Or use 0 for webcam\n",
        "# VIDEO_SOURCE = 0\n",
        "MODEL_PATH = \"Resnet50_Freeze12_LSTM_Transform_Dropout_epoch_6.pth\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEQUENCE_LENGTH = 25\n",
        "THRESHOLD = 0.5\n",
        "print(DEVICE)"
      ],
      "metadata": {
        "id": "0IYyR_4VYLbW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3ec8017-f139-4bf7-b2ce-454addf43758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Model"
      ],
      "metadata": {
        "id": "bJu9w_9HVyn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNLSTM(nn.Module):\n",
        "    def __init__(self, cnn_output_dim = 256, lstm_hidden_dim = 128, num_layers=1):\n",
        "        super(CNNLSTM, self).__init__()\n",
        "\n",
        "        # Use a pre-trained model and strip the classification head\n",
        "        # For Resnet50\n",
        "        base_cnn = models.resnet50(pretrained=True)\n",
        "\n",
        "        # Freeze early layers\n",
        "        for name, param in base_cnn.named_parameters():\n",
        "            if name.startswith(\"conv1\") or name.startswith(\"bn1\") or \\\n",
        "               name.startswith(\"layer1\") or name.startswith(\"layer2\"):\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.cnn = nn.Sequential(*list(base_cnn.children())[:-2],  # keep conv layers\n",
        "                                 nn.AdaptiveAvgPool2d((1, 1)))    # output [B, 512, 1, 1]\n",
        "\n",
        "        self.feature_reduce = nn.Linear(2048, cnn_output_dim)\n",
        "\n",
        "      #   # For Efficientnet_b0\n",
        "      #   base_cnn = models.efficientnet_b0(pretrained=True)\n",
        "      #   # Freeze first 4 blocks (out of 9 total in EfficientNet-B0)\n",
        "      #   for idx, block in enumerate(base_cnn.features):\n",
        "      #       if idx < 4:\n",
        "      #           for param in block.parameters():\n",
        "      #               param.requires_grad = False\n",
        "\n",
        "      #  # Then keep only the convolutional base\n",
        "      #   self.cnn = nn.Sequential(base_cnn.features, nn.AdaptiveAvgPool2d((1, 1)))\n",
        "\n",
        "      #   self.feature_reduce = nn.Linear(1280, cnn_output_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=cnn_output_dim,\n",
        "                            hidden_size=lstm_hidden_dim,\n",
        "                            num_layers=num_layers,\n",
        "                            batch_first=True,\n",
        "                            dropout=0.2 if num_layers > 1 else 0.0)\n",
        "\n",
        "        self.input_dropout = nn.Dropout(p=0.1)\n",
        "        self.hidden_dropout = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.classifier = nn.Linear(lstm_hidden_dim, 1)  # Binary classification\n",
        "        # self.dropout = nn.Dropout(config.dropout)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):  # x: [B, T, 3, 128, 128]\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B * T, C, H, W)  # Flatten for CNN: [B*T, 3, 128, 128]\n",
        "\n",
        "        features = self.cnn(x)      # [B*T, 2048, 1, 1]\n",
        "        features = features.view(B * T, 2048)  # [B*T, 2048]\n",
        "\n",
        "        features = self.input_dropout(features)               # Input dropout\n",
        "        features = self.feature_reduce(features)  # [B*T, cnn_output_dim]\n",
        "        features = features.view(B, T, -1)     # [B, T, cnn_output_dim]\n",
        "\n",
        "        lstm_out, _ = self.lstm(features)      # [B, T, lstm_hidden_dim]\n",
        "        # lstm_out, (h_n, c_n) = self.lstm(features)\n",
        "\n",
        "        # print(lstm_out.size())\n",
        "\n",
        "        last_output = lstm_out[:, -1, :]       # Take last frame's output\n",
        "        # last_hidden = h_n[-1]  # Take last layer’s hidden state [B, hidden_dim]\n",
        "\n",
        "        last_output = self.hidden_dropout(last_output)        # Hidden dropout\n",
        "        out = self.classifier(last_output)     # [B, 1]\n",
        "        # return self.sigmoid(out).squeeze(1)    # [B]\n",
        "        return out.squeeze(1)    # [B]\n",
        "\n",
        "# Hidden state\n",
        "# Output layer"
      ],
      "metadata": {
        "id": "XLrYnC6AV5Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNNLSTM()\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location = DEVICE))\n",
        "model.to(DEVICE)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "Qdib3pMOVyMG",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cc48fc5-fdf9-46a8-96c0-6b304da8c9e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNNLSTM(\n",
              "  (cnn): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (7): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  )\n",
              "  (feature_reduce): Linear(in_features=2048, out_features=256, bias=True)\n",
              "  (lstm): LSTM(256, 128, batch_first=True)\n",
              "  (input_dropout): Dropout(p=0.1, inplace=False)\n",
              "  (hidden_dropout): Dropout(p=0.3, inplace=False)\n",
              "  (classifier): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Frames Preprocessing"
      ],
      "metadata": {
        "id": "PpSeZbk4VZeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),\n",
        "#     transforms.ToTensor(),\n",
        "#     # transforms.Normalize([0.485, 0.456, 0.406],\n",
        "#     #                      [0.229, 0.224, 0.225])\n",
        "# ])"
      ],
      "metadata": {
        "id": "WhcTVGaBVa6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Preprocess Function ---\n",
        "def preprocess_face(face):\n",
        "    face = cv2.resize(face, (224, 224))\n",
        "    face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "    face = F.to_tensor(face)\n",
        "    # face = F.normalize(face, mean=[0.485, 0.456, 0.406],\n",
        "    #                          std=[0.229, 0.224, 0.225])\n",
        "    return face\n",
        "\n",
        "# def preprocess_face(face_img):\n",
        "#     face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n",
        "#     return transform(face_rgb)"
      ],
      "metadata": {
        "id": "qsNkUynkY3Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Video Using OpenCV and Sample Frames"
      ],
      "metadata": {
        "id": "Hr5etkGAuR0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "face_detector = YOLO(\"yolov8m_200e.pt\")"
      ],
      "metadata": {
        "id": "afS_4n0pYxZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output video writer\n",
        "# --- Initialize OpenCV Video ---\n",
        "cap = cv2.VideoCapture(VIDEO_SOURCE)\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(f'output1D_Resnet50_Freeze12_LSTM_Transform_Dropout_epoch_6.mp4', fourcc, 25.0, (int(cap.get(3)), int(cap.get(4))))"
      ],
      "metadata": {
        "id": "Sl66SucFJJOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Without Zoom"
      ],
      "metadata": {
        "id": "k93uTbamM4db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- Initialize OpenCV Video ---\n",
        "# cap = cv2.VideoCapture(VIDEO_SOURCE)\n",
        "\n",
        "# --- Sequence Buffer ---\n",
        "sequence = deque(maxlen=SEQUENCE_LENGTH)\n",
        "\n",
        "tracker = Sort()\n",
        "sequence_dict = {}  # To store per-ID face sequences\n",
        "\n",
        "while cap.isOpened:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"cannot read\")\n",
        "        break\n",
        "\n",
        "    result_list = face_detector(frame, verbose=False)\n",
        "\n",
        "    if len(result_list) == 0:\n",
        "        continue  # Skip this frame if no detection result\n",
        "\n",
        "    results = result_list[0]\n",
        "\n",
        "    if results.boxes is None or len(results.boxes) == 0:\n",
        "        continue  # No faces detected, skip this frame\n",
        "\n",
        "    boxes = results.boxes  # Proceed safely now\n",
        "    detections = []\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "        box = boxes.xyxy[i]\n",
        "        conf = boxes.conf[i]\n",
        "\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        conf = float(conf)\n",
        "        detections.append([x1, y1, x2, y2, conf])\n",
        "\n",
        "    # Track with SORT\n",
        "    tracked_objects = tracker.update(np.array(detections))\n",
        "\n",
        "    for track in tracked_objects:\n",
        "        x1, y1, x2, y2, track_id = map(int, track)\n",
        "        face = frame[y1:y2, x1:x2]\n",
        "\n",
        "        # Preprocess\n",
        "        processed = preprocess_face(face)\n",
        "        sequence = sequence_dict.get(track_id, deque(maxlen=25))\n",
        "        sequence.append(processed)\n",
        "        sequence_dict[track_id] = sequence\n",
        "\n",
        "        # Predict if enough frames collected\n",
        "        if len(sequence) == 25:\n",
        "            input_tensor = torch.stack(list(sequence)).unsqueeze(0).to(DEVICE)\n",
        "            with torch.no_grad():\n",
        "                output = model(input_tensor)\n",
        "                prob = torch.sigmoid(output).item()\n",
        "\n",
        "            label = f\"Speaking ({prob:.2f})\" if prob > THRESHOLD else f\"Not Speaking ({prob:.2f})\"\n",
        "            color = (0, 255, 0) if prob > THRESHOLD else (0, 0, 255)\n",
        "        else:\n",
        "            label = f\"ID {track_id} - Collecting...\"\n",
        "            color = (200, 200, 0)\n",
        "\n",
        "        # Annotate\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "\n",
        "    out.write(frame)\n",
        "    # Optionally show: cv2_imshow(frame)\n",
        "\n",
        "cap.release()\n",
        "out.release()"
      ],
      "metadata": {
        "id": "1OgYSYXHG5mY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e301ee06-1860-4a0f-d41e-82c48e9589a8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cannot read\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### With Zoom"
      ],
      "metadata": {
        "id": "wOWJEQBuM69F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from collections import deque, defaultdict\n",
        "# import numpy as np\n",
        "# import cv2\n",
        "# import torch\n",
        "\n",
        "PAD = 40                  # pixels of padding around the zoom box\n",
        "STICK_FRAMES = 10         # keep current speaker for some frames to avoid flicker\n",
        "\n",
        "sequence_dict = defaultdict(lambda: deque(maxlen=25))\n",
        "last_speaker_id = None\n",
        "stick_counter = 0\n",
        "\n",
        "def safe_zoom(frame, box, pad=40):\n",
        "    \"\"\"Crop padded box and resize back to original size.\"\"\"\n",
        "    h, w = frame.shape[:2]\n",
        "    x1, y1, x2, y2 = map(int, box)\n",
        "    x1 = max(x1 - pad, 0)\n",
        "    y1 = max(y1 - pad, 0)\n",
        "    x2 = min(x2 + pad, w)\n",
        "    y2 = min(y2 + pad, h)\n",
        "    # guard against empty crop\n",
        "    if x2 <= x1 or y2 <= y1:\n",
        "        return frame\n",
        "    roi = frame[y1:y2, x1:x2]\n",
        "    return cv2.resize(roi, (w, h))\n",
        "\n",
        "# --- Initialize OpenCV Video ---\n",
        "cap = cv2.VideoCapture(VIDEO_SOURCE)\n",
        "\n",
        "tracker = Sort()\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"cannot read\")\n",
        "        break\n",
        "\n",
        "    # --- Face detection (YOLO) ---\n",
        "    result_list = face_detector(frame, verbose=False)\n",
        "    if len(result_list) == 0:\n",
        "        out.write(frame)\n",
        "        continue\n",
        "\n",
        "    results = result_list[0]\n",
        "    if results.boxes is None or len(results.boxes) == 0:\n",
        "        out.write(frame)\n",
        "        continue\n",
        "\n",
        "    boxes = results.boxes\n",
        "    detections = []\n",
        "    for i in range(len(boxes)):\n",
        "        box = boxes.xyxy[i]\n",
        "        conf = float(boxes.conf[i])\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        detections.append([x1, y1, x2, y2, conf])\n",
        "\n",
        "    # --- Tracking ---\n",
        "    tracked_objects = tracker.update(np.array(detections))\n",
        "\n",
        "    # Collect per-track probabilities for this frame\n",
        "    frame_probs = {}      # track_id -> prob\n",
        "    frame_labels = {}     # track_id -> label text\n",
        "    frame_colors = {}     # track_id -> color BGR\n",
        "    frame_boxes = {}      # track_id -> (x1,y1,x2,y2)\n",
        "\n",
        "    for tr in tracked_objects:\n",
        "        x1, y1, x2, y2, track_id = map(int, tr)\n",
        "        frame_boxes[track_id] = (x1, y1, x2, y2)\n",
        "\n",
        "        # Preprocess face for your model\n",
        "        face = frame[y1:y2, x1:x2]\n",
        "        processed = preprocess_face(face)           # must match training transforms\n",
        "        seq = sequence_dict[track_id]\n",
        "        seq.append(processed)\n",
        "\n",
        "        if len(seq) == 25:\n",
        "            input_tensor = torch.stack(list(seq)).unsqueeze(0).to(DEVICE)\n",
        "            with torch.no_grad():\n",
        "                output = model(input_tensor)\n",
        "                prob = float(torch.sigmoid(output).item())\n",
        "            frame_probs[track_id] = prob\n",
        "\n",
        "            speaking = prob > THRESHOLD\n",
        "            label = f\"{'Speaking' if speaking else 'Not Speaking'} ({prob:.2f})\"\n",
        "            color = (0, 255, 0) if speaking else (0, 0, 255)\n",
        "        else:\n",
        "            label = f\"ID {track_id} - Collecting...\"\n",
        "            color = (200, 200, 0)\n",
        "\n",
        "        frame_labels[track_id] = label\n",
        "        frame_colors[track_id] = color\n",
        "\n",
        "    # --- Decide who to zoom on ---\n",
        "    zoom_box = None\n",
        "    if frame_probs:\n",
        "        # pick highest prob\n",
        "        cur_speaker = max(frame_probs.items(), key=lambda kv: kv[1])[0]\n",
        "        cur_prob = frame_probs[cur_speaker]\n",
        "\n",
        "        if cur_prob > THRESHOLD:\n",
        "            # if new speaker differs, optionally require few frames (stickiness)\n",
        "            if last_speaker_id is None or cur_speaker == last_speaker_id or stick_counter <= 0:\n",
        "                zoom_box = frame_boxes[cur_speaker]\n",
        "                if cur_speaker != last_speaker_id:\n",
        "                    last_speaker_id = cur_speaker\n",
        "                    stick_counter = STICK_FRAMES\n",
        "            else:\n",
        "                # keep previous speaker for a few frames\n",
        "                if last_speaker_id in frame_boxes:\n",
        "                    zoom_box = frame_boxes[last_speaker_id]\n",
        "        else:\n",
        "            # nobody above threshold\n",
        "            last_speaker_id = None\n",
        "            stick_counter = 0\n",
        "    else:\n",
        "        last_speaker_id = None\n",
        "        stick_counter = 0\n",
        "\n",
        "    if stick_counter > 0:\n",
        "        stick_counter -= 1\n",
        "\n",
        "    # --- Draw boxes/labels BEFORE zoom (so they scale nicely) ---\n",
        "    for tid, (x1, y1, x2, y2) in frame_boxes.items():\n",
        "        color = frame_colors.get(tid, (255, 255, 255))\n",
        "        label = frame_labels.get(tid, f\"ID {tid}\")\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "        cv2.putText(frame, label, (x1, y1 - 8), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "\n",
        "    # --- Zoom to speaker (fallback to full frame if none) ---\n",
        "    if zoom_box is not None:\n",
        "        frame = safe_zoom(frame, zoom_box, pad=PAD)\n",
        "\n",
        "    # write / show\n",
        "    out.write(frame)\n",
        "    # cv2.imshow(\"speaker_detection\", frame)\n",
        "    # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "    #     break\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "# cv2.destroyAllWindows()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QnZoGWSM-UL",
        "outputId": "ee7f6e89-7eeb-4063-c9c1-a9224dea8326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cannot read\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For Opening Webcam on google colab"
      ],
      "metadata": {
        "id": "qFhj_l_vJnBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ],
      "metadata": {
        "id": "fHjfh-3SJ8Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML =\n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "\n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "\n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "\n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "\n",
        "      return {'create': preShow - preCreate,\n",
        "              'show': preCapture - preShow,\n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "  # data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  data = eval_js(f'stream_frame({json.dumps(label_html)}, {json.dumps(bbox)})')\n",
        "  return data"
      ],
      "metadata": {
        "id": "Roxqt4A-JqiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Without Zoom"
      ],
      "metadata": {
        "id": "t7q3vpV790TV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tracker = Sort()\n",
        "sequence_dict = defaultdict(lambda: deque(maxlen=25))\n",
        "\n",
        "# Webcam simulation start\n",
        "video_stream()\n",
        "label_html = 'Capturing...'\n",
        "bbox = ''\n",
        "\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    frame = js_to_image(js_reply[\"img\"])  # RGB\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # Convert to BGR for YOLO\n",
        "\n",
        "    result_list = face_detector(frame, verbose=False)\n",
        "    if len(result_list) == 0 or result_list[0].boxes is None:\n",
        "        continue\n",
        "\n",
        "    results = result_list[0]\n",
        "    boxes = results.boxes\n",
        "    detections = []\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "        box = boxes.xyxy[i]\n",
        "        conf = boxes.conf[i]\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        detections.append([x1, y1, x2, y2, float(conf)])\n",
        "\n",
        "    tracked_objects = tracker.update(np.array(detections))\n",
        "\n",
        "    bbox_array = np.zeros([480, 640, 4], dtype=np.uint8)  # For bounding box overlay\n",
        "\n",
        "    for track in tracked_objects:\n",
        "        x1, y1, x2, y2, track_id = map(int, track)\n",
        "        face = frame[y1:y2, x1:x2]\n",
        "\n",
        "        processed = preprocess_face(face)  # Apply same transform as during training\n",
        "        sequence = sequence_dict[track_id]\n",
        "        sequence.append(processed)\n",
        "\n",
        "        if len(sequence) == 25:\n",
        "            input_tensor = torch.stack(list(sequence)).unsqueeze(0).to(DEVICE)\n",
        "            with torch.no_grad():\n",
        "                output = model(input_tensor)\n",
        "                prob = torch.sigmoid(output).item()\n",
        "\n",
        "            label = f\"Speaking ({prob:.2f})\" if prob > THRESHOLD else f\"Not Speaking ({prob:.2f})\"\n",
        "            color = (0, 255, 0) if prob > THRESHOLD else (0, 0, 255)\n",
        "        else:\n",
        "            label = f\"ID {track_id} - Collecting...\"\n",
        "            color = (200, 200, 0)\n",
        "\n",
        "        cv2.rectangle(bbox_array, (x1, y1), (x2, y2), color, 2)\n",
        "        cv2.putText(bbox_array, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "    bbox_array[:, :, 3] = (bbox_array.max(axis=2) > 0).astype(np.uint8) * 255\n",
        "    bbox = bbox_to_bytes(bbox_array)  # Send to JS overlay"
      ],
      "metadata": {
        "id": "aW91r7sGK2jd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "20e2f900-1a07-41b7-beff-f072a90816db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "\n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "\n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "\n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "\n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "\n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "\n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML =\n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640; //video.videoWidth;\n",
              "      captureCanvas.height = 480; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "\n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "\n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "\n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "\n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "\n",
              "      return {'create': preShow - preCreate,\n",
              "              'show': preCapture - preShow,\n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-480462610.py:27: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### With Zoom"
      ],
      "metadata": {
        "id": "NgyNwI7e92jG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tracker = Sort()\n",
        "sequence_dict = defaultdict(lambda: deque(maxlen=25))\n",
        "\n",
        "# Webcam simulation start\n",
        "video_stream()\n",
        "label_html = 'Capturing...'\n",
        "bbox = ''\n",
        "\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    frame = js_to_image(js_reply[\"img\"])  # RGB\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # Convert to BGR for YOLO\n",
        "\n",
        "    result_list = face_detector(frame, verbose=False)\n",
        "    if len(result_list) == 0 or result_list[0].boxes is None:\n",
        "        continue\n",
        "\n",
        "    results = result_list[0]\n",
        "    boxes = results.boxes\n",
        "    detections = []\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "        box = boxes.xyxy[i]\n",
        "        conf = boxes.conf[i]\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        detections.append([x1, y1, x2, y2, float(conf)])\n",
        "\n",
        "    tracked_objects = tracker.update(np.array(detections))\n",
        "\n",
        "    # bbox_array = np.zeros([480, 640, 4], dtype=np.uint8)  # For bounding box overlay\n",
        "\n",
        "    # After processing all tracks\n",
        "    # Find the person with the highest speaking prob\n",
        "    zoom_face = None\n",
        "    max_prob = 0\n",
        "    zoom_box = None\n",
        "\n",
        "    for track in tracked_objects:\n",
        "        x1, y1, x2, y2, track_id = map(int, track)\n",
        "        face = frame[y1:y2, x1:x2]\n",
        "        processed = preprocess_face(face)  # Apply same transform as during training\n",
        "        sequence = sequence_dict[track_id]\n",
        "        sequence.append(processed)\n",
        "        if len(sequence) == 25:\n",
        "            input_tensor = torch.stack(list(sequence)).unsqueeze(0).to(DEVICE)\n",
        "            with torch.no_grad():\n",
        "                output = model(input_tensor)\n",
        "                prob = torch.sigmoid(output).item()\n",
        "\n",
        "            if prob > THRESHOLD and prob > max_prob:\n",
        "                max_prob = prob\n",
        "                zoom_box = (x1, y1, x2, y2)\n",
        "\n",
        "            label = f\"Speaking ({prob:.2f})\" if prob > THRESHOLD else f\"Not Speaking ({prob:.2f})\"\n",
        "            color = (0, 255, 0) if prob > THRESHOLD else (0, 0, 255)\n",
        "        else:\n",
        "            label = f\"ID {track_id} - Collecting...\"\n",
        "            color = (200, 200, 0)\n",
        "\n",
        "        # cv2.rectangle(bbox_array, (x1, y1), (x2, y2), color, 2)\n",
        "        # cv2.putText(bbox_array, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "    # Simulate zoom if a speaker was detected\n",
        "    if zoom_box:\n",
        "        x1, y1, x2, y2 = zoom_box\n",
        "\n",
        "        # Add padding around the face box for better framing\n",
        "        h, w, _ = frame.shape\n",
        "        pad = 40\n",
        "        x1 = max(x1 - pad, 0)\n",
        "        y1 = max(y1 - pad, 0)\n",
        "        x2 = min(x2 + pad, w)\n",
        "        y2 = min(y2 + pad, h)\n",
        "\n",
        "        # # Crop and resize to full frame size\n",
        "        # face_roi = frame[y1:y2, x1:x2]\n",
        "        # zoomed = cv2.resize(face_roi, (640, 480))  # Resize to full frame\n",
        "        # frame = zoomed  # Replace the original frame with zoomed face\n",
        "\n",
        "        # # Smooth transition: blend current frame and zoomed face\n",
        "        # alpha = 0.9  # Adjust for smoothness (0 = only original, 1 = only zoom)\n",
        "        # frame = cv2.addWeighted(frame, 1 - alpha, zoomed, alpha, 0)\n",
        "\n",
        "        face_roi = frame[y1:y2, x1:x2]\n",
        "\n",
        "        # --- Center the zoomed face in the frame ---\n",
        "        face_h, face_w, _ = face_roi.shape\n",
        "        zoomed_frame = np.zeros_like(frame)  # black background\n",
        "\n",
        "        # Resize face ROI to fit a portion of the screen (optional: full 640x480)\n",
        "        resized_face = cv2.resize(face_roi, (min(640, face_w * 2), min(480, face_h * 2)))\n",
        "\n",
        "        # Compute top-left corner to center it\n",
        "        rf_h, rf_w, _ = resized_face.shape\n",
        "        start_y = (480 - rf_h) // 2\n",
        "        start_x = (640 - rf_w) // 2\n",
        "\n",
        "        # Place resized face in center of black frame\n",
        "        zoomed_frame[start_y:start_y + rf_h, start_x:start_x + rf_w] = resized_face\n",
        "        frame = zoomed_frame  # Replace original frame\n",
        "\n",
        "    else:\n",
        "        # 🛡️ No speaker detected: show original frame\n",
        "        pass  # Keep full frame (no zoom)\n",
        "\n",
        "    # --- Convert updated frame to base64 image and send to JS for display ---\n",
        "    im_pil = Image.fromarray(frame)\n",
        "    buff = BytesIO()\n",
        "    im_pil.save(buff, format=\"jpeg\")\n",
        "    frame_bytes = b64encode(buff.getvalue()).decode(\"utf-8\")\n",
        "    label_html = f'<img src=\"data:image/jpeg;base64,{frame_bytes}\"/>'\n",
        "\n",
        "    # bbox_array[:, :, 3] = (bbox_array.max(axis=2) > 0).astype(np.uint8) * 255\n",
        "    # bbox = bbox_to_bytes(bbox_array)  # Send to JS overlay"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Bv8V-uQBSIZN",
        "outputId": "277717d1-2f12-417b-fbb0-a2c305b62691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "\n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "\n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "\n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "\n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "\n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "\n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML =\n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640; //video.videoWidth;\n",
              "      captureCanvas.height = 480; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "\n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "\n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "\n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "\n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "\n",
              "      return {'create': preShow - preCreate,\n",
              "              'show': preCapture - preShow,\n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}